{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmelende/CS5262-50_Project/blob/main/Copy_of_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 4**\n",
        "\n",
        "Cory Melendez"
      ],
      "metadata": {
        "id": "T1tmK91H2bCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background:\n",
        "One of the most coveted types of prediction, maybe only second to knowing the lottery numbers, is that of the stock market. Stock traders, that is Non HFT/algo traders, will typically look to tools such as quantitative analysis (QA). They attempt to detect patterns that arise from human behavior, believed to be unchanging. This project instead attempts to use ML models in order to build up a scientifically more sound method than methods such as QA that can predict how the market will move in the very short term.\n",
        "\n",
        "One of the challenges to this project, and will require further research, is to identity not only if our model will have an acceptable accuracy using the data below, but why. Markets are ever-changing, and are constantly affected not only by geoeconomic events, but geopolitical events as well. Identifying the circumstances where it *could* work and *when* it could work is just as important as building the model itself. We found paper 1, which goes over how researchers used this dataset, and paper 2, a summary of 100 published articles, to be a good starting point in understanding what we will call the 'reproducability' problem as well as understanding how to build our models better.\n",
        "\n",
        "The last challenge is the size of the data, I'll be trimming the number of columns not only to stay under the column limit, but to better narrow down and understand the factors that contribute to the output.\n",
        "\n",
        "\n",
        "\n",
        "### Project Description: \n",
        "This project's main aim is to output a binary classificiation that will be able to tell a theoritical day trader whether or not they should buy and sell a stock on a certain day.This project will use data from link below in order to build our ML models. We will also refer to the paper below in order for not only reference on how a model can be built, but to also refer to it for understanding the domain.\n",
        "\n",
        "Our main methodology will be to groom the data into something that is digestible by the model. Since our aim is to only output a binary classification (yes/no), the question we will be asking our model is 'should i buy the stock at close today and sell at close of tomorrow'. This simple strategy will simplify the model greatly, but will require restructuring the data and creating a target column that our model predicts.\n",
        "\n",
        "Our simple goals that define success is as follows: Can we beat not entering the market at all (not investing)? Can our model beat a trading algorithm that simply randomly outputs yes/no? A goal beyond that would be to answer: What other non-ML algorithms, in the same time and circumstances, can our model beat?\n",
        "\n",
        "\n",
        "Data: https://archive.ics.uci.edu/ml/datasets/CNNpred%3A+CNN-based+stock+market+prediction+using+a+diverse+set+of+variables\n",
        "\n",
        "Paper: https://reader.elsevier.com/reader/sd/pii/S0957417419301915?token=746C33D1046F2DDA4EE614C2A4606AF2260493F0DC652081FF1F03968E01DC023369A293A638CEE35E24DB8BB7EE1259&originRegion=us-east-1&originCreation=20230119051158\n",
        "\n",
        "Paper 2: https://reader.elsevier.com/reader/sd/pii/S0957417421009441?token=EEE8A8BF467B1370F99F73C46DC7AD74D88F0AB17AC08F5137A3B16E14E69D382DE8A689EF492C09E0CF67690D289807&originRegion=us-east-1&originCreation=20230119052216\n",
        "\n",
        "### Performance Metric: \n",
        "\n",
        "Our metric will be pretty simple, given any trade, or a group of trades, advised by the model, is: \n",
        "\n",
        "(current price - bought price ) * X > X * epsilon, where X is the amount of money that was invested and epsilon is any arbitrary small positive number\n",
        "\n",
        " \n",
        "If this statement is true, then our model has outperformed our money not entering the market at all by an arbitrary small number.\n",
        "\n",
        "Furthermore, \n",
        "\n",
        "does:\n",
        "\n",
        "Sum(MLProfit) > Sum(RandomProfit) * epsilon\n",
        "\n",
        "Where Sum(MLProfit) is the profit made from a arbitrary number of trades using our ML model greater, and Sum(RandomProfit) is the profit madae from an arbitrary number of trades using a random output algorithm, and epsilon is an arbitrary small number\n",
        "\n",
        "If so, then we have beaten randomness by a factor of epsilon and have met our main objective.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4NMWjlceyZ7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Notebook"
      ],
      "metadata": {
        "id": "rsO9TMjmjNL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import sklearn as skl\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "from numpy.random import randint\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn import config_context\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n"
      ],
      "metadata": {
        "id": "gARu32GQi9aK"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dji_file = 'Processed_DJI.csv'\n",
        "nasdaq_file = 'Processed_NASDAQ.csv'\n",
        "nyse_file = 'Processed_NYSE.csv'\n",
        "russel_file = 'Processed_RUSSELL.csv'\n",
        "sp_file = 'Processed_S&P.csv'\n",
        "\n",
        "dji_df = pd.read_csv(dji_file)\n",
        "nasdaq_df= pd.read_csv(nasdaq_file)\n",
        "nyse_df = pd.read_csv(nyse_file);\n",
        "russel_df = pd.read_csv(russel_file);\n",
        "sp_df = pd.read_csv(sp_file);\n"
      ],
      "metadata": {
        "id": "tn-jk6b1AGZX"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning and Feature Transformers\n",
        "Combine, remove, and create columns to give our model a better context for each trading day and how the market reacted on previous' days"
      ],
      "metadata": {
        "id": "QzqUxoYWBQ7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GLOBALS\n",
        "random_seed = 42\n",
        "OIL= \"Oil\"\n",
        "GOLD=\"Gold\"\n",
        "XOM=\"XOM\"\n",
        "JPM=\"JPM\"\n",
        "GE=\"GE\"\n",
        "JNJ=\"JNJ\"\n",
        "WFC=\"WFC\"\n",
        "AMZN=\"AMZN\"\n",
        "MSFT = \"MSFT\"\n",
        "\n",
        "STOCK_COLS: List[str] = [\n",
        "    OIL, GOLD, XOM, JPM, GE, JNJ, WFC, AMZN, MSFT\n",
        "]\n",
        "\n",
        "DTB4WK = \"DTB4WK\"\n",
        "DTB3 = \"DTB3\"\n",
        "DTB6 = \"DTB6\"\n",
        "DGS5 = \"DGS5\"\n",
        "DGS10 = \"DGS10\"\n",
        "\n",
        "\n",
        "TREASURY_COLS: List[str] = [\n",
        "    DTB4WK, DTB3, DTB6, DGS5, DGS10\n",
        "]\n",
        "\n",
        "STOCKS_BY_INDUSTRY: Dict[str, str] = {\n",
        "    OIL: 'oil & gas',\n",
        "    GOLD: 'industrial metal',\n",
        "    XOM: 'oil & gas',\n",
        "    JPM: 'banking',\n",
        "    GE: 'industrial machinery',\n",
        "    JNJ: 'drugs',\n",
        "    WFC: 'banking',\n",
        "    AMZN: 'tech',\n",
        "    MSFT: 'tech'\n",
        "}\n",
        "#['oil & gas', 'industrial metal', 'Banking', 'industrial machinery', 'drugs', ]\n",
        "COMMON_COLS: List[str] = [\n",
        "    'Date', 'Close', 'Volume', \n",
        "    #Add this for convenience\n",
        "    'Name'\n",
        "] + TREASURY_COLS + STOCK_COLS\n",
        "\n",
        "#columns that need to be calculated\n",
        "CALC_COLS = [\n",
        "    #dow jones\n",
        "    'DJI',\n",
        "    # ny stock exchang\n",
        "    'NYSE', \n",
        "    #russel\n",
        "    'RUSSEL', \n",
        "    #s&p 500\n",
        "    'GSPC', \n",
        "    #nasdaq\n",
        "    'IXIC'\n",
        "]\n",
        "\n",
        "ALL_DFS: List[pd.DataFrame] = [dji_df, nasdaq_df, nyse_df, russel_df, sp_df]"
      ],
      "metadata": {
        "id": "Frv6d4jgNLPK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "hhD7cuY3bW4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the rate of change from the previous two days and drop the first day\n",
        "class DateToQuarterTransformer(BaseEstimator, TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "\n",
        "    X_['Date'] = pd.to_datetime(X_['Date'])\n",
        "    for i in range(0, len(X_)):\n",
        "      month = X_.loc[i, 'Date'].month\n",
        "\n",
        "      if 1 <= month <= 3:\n",
        "        X_.loc[i, 'Quarter'] = 1;\n",
        "      elif 4 <= month <= 6:\n",
        "        X_.loc[i, 'Quarter'] = 2;\n",
        "      elif 7 <= month <= 9:\n",
        "        X_.loc[i, 'Quarter'] = 3\n",
        "      else:\n",
        "        X_.loc[i, 'Quarter'] = 4\n",
        "\n",
        "    X_.drop(columns=['Date'], inplace=True)\n",
        "    return X_\n",
        "\n",
        "class DateTransformer(BaseEstimator, TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "  \n",
        "  def transform(self, X, y=None):\n",
        "    X_=X.copy()\n",
        "    X_['Date'] = pd.to_datetime(X_['Date'])\n",
        "    X_['Year'] = X_['Date'].dt.year\n",
        "    X_['Month'] = X_['Date'].dt.month;\n",
        "    X_['DayOfMonth'] = X_['Date'].dt.day;\n",
        "    X_['DayOfWeek'] = X_['Date'].dt.dayofweek\n",
        "\n",
        "    X_.drop(columns=['Date'], inplace=True)\n",
        "    return X_\n",
        "\n",
        "class TrimUnusedColumnsTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def __init__(self, keepColumns: List[str]):\n",
        "    self.keepColumns: List[str] = keepColumns\n",
        "  \n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    X_ = X_[self.keepColumns]\n",
        "    return X_\n",
        "\n",
        "class RenameCloseVolumeColumnsTransformer(BaseEstimator, TransformerMixin):\n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_=X.copy()\n",
        "    exchangeName = X.loc[0, 'Name']\n",
        "    return X_.rename(columns={'Close': f'{exchangeName}_Close', 'Volume': f'{exchangeName}_Volume'})\n",
        "\n",
        "class TreasuryRateTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, targetTreasury: str, backNumberOfDays: int):\n",
        "    self.targetTreasury: str = targetTreasury;\n",
        "    self.backNumberOfDays: int = backNumberOfDays\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_= X.copy()\n",
        "\n",
        "    for i in range(self.backNumberOfDays, len(X)):\n",
        "        X_.loc[i, f'{self.targetTreasury}_Last_{self.backNumberOfDays}_Yield_Change'] = X_.loc[i, self.targetTreasury] - X_.loc[i - self.backNumberOfDays, self.targetTreasury]\n",
        "\n",
        "    return X_   \n",
        "\n",
        "class DaysBackRateTransformer(BaseEstimator, TransformerMixin): \n",
        "\n",
        "  def __init__(self, targetStock: str, numberOfDays: int):\n",
        "    self.targetStock: str = targetStock\n",
        "    self.numberOfDays: int = numberOfDays\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    return self.addTargetStockDayRate(X_, self.targetStock)\n",
        "\n",
        "  def addTargetStockDayRate(self, X_: pd.DataFrame, stock: str):\n",
        "    for i in range(self.numberOfDays, len(X_)):\n",
        "      X_.loc[i, f'{stock}_-{self.numberOfDays}_DayRate'] = X_.loc[i - self.numberOfDays, stock]\n",
        "    return X_\n",
        "\n",
        "\n",
        "class TotalChangeOverDaysTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, overDays: int, targetStock: str):\n",
        "    self.overDays = overDays\n",
        "    self.targetStock = targetStock;\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    return self.addTotalChange(X_, self.targetStock)\n",
        "  \n",
        "  def addTotalChange(self, X_: pd.DataFrame, stock: str):\n",
        "    for i in range(self.overDays - 1, len(X_)):\n",
        "      X_.loc[i, f'{stock}_TotalChange_Last_{self.overDays}_Days'] = self.phi(X_.loc[i-self.overDays + 1:i, stock].values.tolist())\n",
        "    return X_;\n",
        "\n",
        "  def phi(self, numbers):\n",
        "    product = 1\n",
        "    for n in numbers:\n",
        "      mult = (1+n)\n",
        "      if mult == 1:\n",
        "        mult = 1\n",
        "\n",
        "      product *= mult\n",
        "    return product\n",
        "\n",
        "class WouldProfitTargetTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, targetStock: str):\n",
        "    self.targetStock = targetStock\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_=X.copy()\n",
        "    for i in range(0, len(X)-1):\n",
        "      if(X_.loc[i+1, f'{self.targetStock}'] > 0):\n",
        "        X_.loc[i, f'{self.targetStock}_WouldProfit'] = 'invest'\n",
        "      else:\n",
        "        X_.loc[i, f'{self.targetStock}_WouldProfit'] = 'abstain'\n",
        "    return X_\n",
        "\n",
        "class AddExchangeCloseVolumeTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, source: pd.DataFrame):\n",
        "    self.source = source\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    exchangeName = self.source.loc[0,'Name']\n",
        "    X_[f'{exchangeName}_Close'] = self.source[f'Close']\n",
        "    X_[f'{exchangeName}_Volume'] = self.source[f'Volume']\n",
        "    return X_\n",
        "\n",
        "class SameIndustryDaysBackRateTransformer(DaysBackRateTransformer):\n",
        "  def __init__(self, industryByStock: Dict[str, str], targetStock: str, numberOfDays: int):\n",
        "    super().__init__(targetStock, numberOfDays)\n",
        "    self.industryByStock = industryByStock\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    targetStockIndustry = self.industryByStock[self.targetStock]\n",
        "\n",
        "    for key in self.industryByStock:\n",
        "      if(self.industryByStock[key] == targetStockIndustry and key != self.targetStock):\n",
        "        X_ = self.addTargetStockDayRate(X_, key)\n",
        "\n",
        "    return X_\n",
        "\n",
        "class SameIndustryTotalChangeOverDaysTransformer(TotalChangeOverDaysTransformer):\n",
        "  def __init__(self, industryByStock: Dict[str,str], overDays: int, targetStock: str):\n",
        "    super().__init__(overDays, targetStock)\n",
        "    self.industryByStock = industryByStock\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self;\n",
        "  \n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    targetStockIndustry = self.industryByStock\n",
        "\n",
        "    for key in self.industryByStock:\n",
        "      if(self.industryByStock[key] == targetStockIndustry and key != self.targetStock):\n",
        "        X_ = self.addTotalChange(X_, self.targetStock)\n",
        "\n",
        "    return X_\n",
        "\n",
        "class ExchangeVolumeChangeOverTime(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, exchangeName: str, overDays: int):\n",
        "    self.exchangeName = exchangeName\n",
        "    self.overDays = overDays\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    return self.addTotalChange(X)\n",
        "\n",
        "  def addTotalChange(self, X_: pd.DataFrame):\n",
        "    for i in range(self.overDays - 1, len(X_)):\n",
        "      X_.loc[i, f'{self.exchangeName}_Volume_Change_Over_{self.overDays}_Days'] = self.phi(X_.loc[i-self.overDays + 1:i, f'{self.exchangeName}_Volume'].values.tolist())\n",
        "    return X_;\n",
        "\n",
        "  def phi(self, numbers):\n",
        "    product = 1\n",
        "    for n in numbers:\n",
        "      product *= (1+n)\n",
        "    return product\n",
        "\n",
        "class ExchangeCloseRateChange(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, exchangeName: str, overDays: int):\n",
        "    self.exchangeName = exchangeName\n",
        "    self.overDays = overDays\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "\n",
        "    for i in range(self.overDays, len(X_)):\n",
        "      X_.loc[i, f'{self.exchangeName}_Close_Rate_Change_Over_{self.overDays}_Days'] = 1 - (X_.loc[i, f'{self.exchangeName}_Close'] / X_.loc[i-self.overDays, f'{self.exchangeName}_Close'])\n",
        "\n",
        "    return X_\n",
        "\n",
        "class FinalColCleanup(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  def transform(self, X, y=None):\n",
        "    X.drop(columns=['Name'], inplace=True)\n",
        "    return X\n",
        "\n",
        "class FeatureEngineeringPipelineConstants():\n",
        "  def __init__(self, stockSymbols: List[str], treasurySymbols: List[str], common_cols: List[str], industryByStock: Dict[str,str]):\n",
        "    self.stockSymbols: List[str] = stockSymbols\n",
        "    self.treasurySymbols: List[str] = treasurySymbols\n",
        "    self.commonColumns: List[str] = common_cols\n",
        "    self.industryByStock: Dict[str, str] = industryByStock\n",
        "\n",
        "class FeatureEngineeringPipelineConfig():\n",
        "  def __init__(self, dataframes: List[pd.DataFrame], constants: FeatureEngineeringPipelineConstants, daysBack: List[int], targetStock: str):\n",
        "    self.constants: FeatureEngineeringPipelineConstants = constants\n",
        "\n",
        "    self.dataframes: List[pd.DataFrames] = dataframes\n",
        "    self.targetStock: str = targetStock\n",
        "    self.daysBack = daysBack\n",
        "\n",
        "class FeatureEngineeringPipelineFactory():\n",
        "  def __init__(self, config: FeatureEngineeringPipelineConfig):\n",
        "    self.config = config\n",
        "\n",
        "  def __createAddExchangeCloseVolumeTransformers(self):\n",
        "    l = []\n",
        "\n",
        "    for i in range(0, len(self.config.dataframes)):\n",
        "      l.append((f'use_add_df{i}_exchange_transformer', AddExchangeCloseVolumeTransformer(self.config.dataframes[i])))\n",
        "\n",
        "    return l\n",
        "\n",
        "  def __createDaysBackRateTransformers(self): \n",
        "    l = []\n",
        "\n",
        "    for stock in self.config.constants.stockSymbols:\n",
        "      for n in self.config.daysBack:\n",
        "        l.append((f'use_{n}_days_back_rate_transformer_{stock}', DaysBackRateTransformer(stock, n)))\n",
        "\n",
        "    return l\n",
        "\n",
        "  def __createExchangeCloseRateChange(self):\n",
        "    l = []\n",
        "\n",
        "    for df in self.config.dataframes:\n",
        "      exchangeName = df.loc[0, 'Name']\n",
        "      for n in self.config.daysBack:\n",
        "        l.append((f'use_exchange_{exchangeName}_close_rate_over_{n}_days_transformer', ExchangeCloseRateChange(exchangeName, n)))\n",
        "\n",
        "    return l\n",
        "\n",
        "  def __createExchangeVolumeChangeOverTimeTransformers(self):\n",
        "    l = []\n",
        "\n",
        "    for df in self.config.dataframes:\n",
        "      exchangeName = df.loc[0, 'Name']\n",
        "      for n in self.config.daysBack:\n",
        "        l.append((f'use_exchange_{exchangeName}_volume_change_over_{n}_days', ExchangeVolumeChangeOverTime(exchangeName, n)))\n",
        "    \n",
        "    return l\n",
        "\n",
        "  def __createTotalChangeOverDaysTransformers(self):\n",
        "    l = []\n",
        "\n",
        "    days = self.config.daysBack\n",
        "    for stock in self.config.constants.stockSymbols:\n",
        "      for n in days:\n",
        "        l.append((f'use_total_change_over_{n}_day_transformer_{stock}', TotalChangeOverDaysTransformer(n, stock)))\n",
        "\n",
        "    return l\n",
        "  def __createTreasuryYieldRateChangeTransformers(self):\n",
        "    l = []\n",
        "\n",
        "    for treasury in self.config.constants.treasurySymbols:\n",
        "      for n in self.config.daysBack: \n",
        "        l.append((f'use_treasury_{n}_yield_rate_change_{treasury}', TreasuryRateTransformer(treasury, n)))\n",
        "    \n",
        "    return l\n",
        "\n",
        "  def create(self) -> skl.pipeline.Pipeline:\n",
        "    cleanupSteps = [\n",
        "      (\"use_trim_unused_columns_transformer\", TrimUnusedColumnsTransformer(self.config.constants.commonColumns)),\n",
        "      # (\"use_date_transformer\", DateTransformer()),\n",
        "      (\"use_date_to_quarter_transformer\", DateToQuarterTransformer()),\n",
        "      (\"use_rename_close_volume_columns_tranformer\", RenameCloseVolumeColumnsTransformer()),\n",
        "    ] \n",
        "\n",
        "    addExchangeCloseVolumeTransformers = self.__createAddExchangeCloseVolumeTransformers() \n",
        "\n",
        "    exchangeCloseRateChangeTransformers = self.__createExchangeCloseRateChange()\n",
        "\n",
        "    exchangeVolumeChangeOverTimeTransformers = self.__createExchangeVolumeChangeOverTimeTransformers()\n",
        "\n",
        "    daysBackRateTransformers = self.__createDaysBackRateTransformers()\n",
        "\n",
        "    totalChangeOverDaysTransformers = self.__createTotalChangeOverDaysTransformers()\n",
        "\n",
        "    treasuryRateTransformer = self.__createTreasuryYieldRateChangeTransformers()\n",
        "\n",
        "    targetTransformer = [('use_would_profit_target_transformer', WouldProfitTargetTransformer(self.config.targetStock))]\n",
        "\n",
        "    finalCleanup = [('use_final_col_cleanup_transformer', FinalColCleanup())]\n",
        "\n",
        "    pipelineSteps = cleanupSteps + addExchangeCloseVolumeTransformers + exchangeCloseRateChangeTransformers + exchangeVolumeChangeOverTimeTransformers + daysBackRateTransformers + totalChangeOverDaysTransformers + treasuryRateTransformer + targetTransformer + finalCleanup\n",
        "\n",
        "    return Pipeline(\n",
        "        steps=pipelineSteps\n",
        "    )\n",
        "\n",
        "class FeatureEngineeringPipelineFactoryFactory():\n",
        "  def __init__(self, constants: FeatureEngineeringPipelineConstants, additionalExchangeDfs: List[pd.DataFrame]):\n",
        "    self.additionalExchangeDfs = additionalExchangeDfs\n",
        "    self.constants = constants\n",
        "\n",
        "  def create(self, targetStock: str, numberOfBackDays: List[int]) -> FeatureEngineeringPipelineFactory:\n",
        "    constants = FeatureEngineeringPipelineConstants(STOCK_COLS, TREASURY_COLS, COMMON_COLS, STOCKS_BY_INDUSTRY)\n",
        "    config = FeatureEngineeringPipelineConfig(self.additionalExchangeDfs, constants, numberOfBackDays, targetStock)\n",
        "\n",
        "    return FeatureEngineeringPipelineFactory(config)\n",
        "  def createMany(self, targetStocks: List[str], numberOfBackDayCombinations: List[List[int]]) -> Dict[str, FeatureEngineeringPipelineFactory]:\n",
        "    ret: Dict[str, FeatureEngineeringPipelineFactory] = {}\n",
        "\n",
        "    for stock in targetStocks:\n",
        "      for n in numberOfBackDayCombinations:\n",
        "        ret[stock] = self.create(stock, n)\n",
        "    \n",
        "    return ret\n",
        "  "
      ],
      "metadata": {
        "id": "VZyUqXyfSkt3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "constants = FeatureEngineeringPipelineConfig(STOCK_COLS, TREASURY_COLS, COMMON_COLS, STOCKS_BY_INDUSTRY)\n",
        "initial_exchange = nyse_df\n",
        "additional_exchange_dfs: List[pd.DataFrame] = [dji_df, nasdaq_df, russel_df, sp_df]\n",
        "\n",
        "targetStocks: List[str] = STOCK_COLS + TREASURY_COLS\n",
        "numberOfBackDayCombinations: List[List[int]] = [[1,2,3,5,10]]\n",
        "\n",
        "factoryFactory = FeatureEngineeringPipelineFactoryFactory(constants, additional_exchange_dfs)\n",
        "factories: Dict[str, FeatureEngineeringPipelineFactory] = factoryFactory.createMany(targetStocks, numberOfBackDayCombinations)\n",
        "\n",
        "featureEngPipelines: Dict[str, skl.pipeline.Pipeline] = {}\n",
        "for stock in targetStocks:\n",
        "  featureEngPipelines[stock] = factories[stock].create()"
      ],
      "metadata": {
        "id": "v4SK2ZAR_-Qk"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of resulting transformed dataset\n",
        "\n",
        "Now from above, we have a set of pipelines in 'pipelines' that we can fit on our initial dataframe.\n",
        "\n",
        "I went ahead and created multiple pipelines and datasets, where each one has a different target stock. The one I will be using moving forward will be microsoft stock.\n",
        "\n"
      ],
      "metadata": {
        "id": "TzNy13HOjRpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering\n",
        "\n",
        "From the last assignment, these were the features we were planning on engineering, however, after the extensive transformations above, some have changed or be removed completly.\n",
        "\n",
        "* Research our stock symbols and decide what industry they belong to, add a column indicating the industry. This will be later used for a one hot encode\n",
        "  * Removed:\n",
        "    * In our dataset, we dont have have sufficient number of stocks to warrant this. We only a few industries that had more than one stock.\n",
        "    * We added transformers that calculate the rate change of all stocks across several days. The model should be able to derive which stocks are correlated on itself without adding data to signal what industry that stock is in.\n",
        "\n",
        "* Since this project would normally use a time series approach, we'll want to add additional columns to get around this. Each row has to be indepedent of the previous rows, so we will add the return % of one or several of the previous days to simulate time series. Since in our business context, we are trading on behalf of a day trader, we dont have to add many of these columns\n",
        "  * Added: Added multiple columns signaling the % change or the total change (phi of % changes that were in the dataset).\n",
        "\n",
        "* We will drop all features that are not listed in our data_dictionary.csv as the first step in our pipeline\n",
        "\n",
        "  * Removed some columns from our data_dictionary due to some innaccuracies in the paper that was supplied by the researchers. Not quite sure why, but Gas, Silver, & Copper were described to be in the datasets but were absent.\n",
        "\n",
        "* We'll have to massage our data a little bit. We know that there are some columns whose data is repeated across the CSVs (like stock prices and commodoties) so we can extract that information from just one of the CSVs. Then, we'll want to transform the rest of the columns (ex: close/volume) into multiple columns, so we have NYSE_Close, NYSE_Volume, NASDAQ_Close, NASDAQ_Volume, etc., so we dont have multiple rows representing the same day\n",
        "  * Added: The resulting transformer that we created in this notebook wasnt changed much from the original description.\n",
        "\n",
        "* We'll also need to extract the return % of each stock exchange, the data is a little weird where the % return for a stock exchange x is not in that csv for x. Example: Nasdaq doesnt have the % return for the nasdaq but has NYSE, S&P, etc., we'll want to make sure all % returns our in our dataset.\n",
        "  * This one was a little odd, but our originaly analysis was accurate to the resulting transformer that we needed in this notebook\n",
        "\n",
        "* Further investigation is also needed to combine and average: \n",
        "  1. The % return of each stock exchange\n",
        "    * Modified: Instead of combining or averaging, we added columns to give each row more context as to what the change in the Close price was over various amounts of days\n",
        "  2. Volume\n",
        "    * Modified: Instead of combining or averaging, we added columns to give each row more context as what the cumulative change over various days were\n",
        "  3. Closing price\n",
        "    * Modified: This was merged with #1\n",
        "* Lastly, we'll want to transform the date column into something that we can sort by if needed. We can transform this column from a format 'mm-dd-yyyy' to a number yyyymmdd which will rank by year first, then month, then day\n",
        "  * Modified: Instead of a date, we kept our index and split it across multiple columns (day, month, year, day of week). My thinking was that some traders would trade differently in different parts of the year, and even on different days of the week.\n",
        "\n",
        "#### Final list of features\n",
        "\n",
        "1. DateToQuarterTransformer: transform the date into a quarter, that we'll later use a oneHotEncoder on\n",
        "\n",
        "2. TreasuryRateTransformer: get the rate of change for treasury rates using today and x days ago\n",
        "\n",
        "3. DaysBackRateTransformer: put the rate of change from x amount of days ago for our stocks\n",
        "\n",
        "4. TotalChangeOverDaysTransformer: get the total % of change over last x amount of days for all of our stocks\n",
        "\n",
        "5. WouldProfitTargetTransformer: Add a target column by looking one day into the future to see if the stock went up\n",
        "\n",
        "6. ExchangeVolumeChangeOverTime: calculate the % rate of change for volume for every exchagne\n",
        "\n",
        "7. ExchangeCloseRateChange: calculate the % rate of change for Close price for every exchange."
      ],
      "metadata": {
        "id": "ews1xYhn0Fdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EDA post processing\n",
        "\n",
        "There wasnt a whole lot to be gathered from our EDA from our datasets without the feature engineering we have done in this notebook. But ill include what we did since there were some correlated columns that exist that we can remove along with their corresponding dervied feature columns."
      ],
      "metadata": {
        "id": "yoz-uDdJ0KYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RemoveBadRowsTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    X_ = X_.drop(X_.tail(1).index)\n",
        "    X_ = X_.drop(X_.head(15).index)\n",
        "    return X_ \n",
        "\n",
        "class RemoveHighlyCorrCols(BaseEstimator, TransformerMixin): \n",
        "  def __init__(self, aboveOrAt: float):\n",
        "    self.aboveOrAt = aboveOrAt\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    X_ = X.copy()\n",
        "    _corr = X_.corr()\n",
        "\n",
        "    #find the columns with the highest correlation values and remove\n",
        "    _corr = _corr.abs()\n",
        "    _sol = (_corr.where(np.triu(np.ones(_corr.shape), k=1).astype(bool))\n",
        "                      .stack()\n",
        "                      .sort_values(ascending=True))\n",
        "    \n",
        "    #find highly correlated features\n",
        "    l: set = set()\n",
        "    for index, value in _sol.items():\n",
        "      if value >= self.aboveOrAt and index[0] != index[1]:\n",
        "        if index[0] not in l:\n",
        "          l.add(index[0])\n",
        "        # if index[1] not in l:\n",
        "        #   l.add(index[1])\n",
        "    \n",
        "    remove = list(l)\n",
        "    X_ = X_.drop(columns=remove)\n",
        "    return X_\n",
        "  \n",
        "class ExcludeColsStandardScaler(BaseEstimator, TransformerMixin): \n",
        "    def __init__(self, excludeColumns: List[str]):\n",
        "      self.excludeColumns = excludeColumns\n",
        "    \n",
        "    def fit(self, X, y = None):\n",
        "      performOnSet: set = set()\n",
        "      for col in X.columns:\n",
        "        if col not in self.excludeColumns:\n",
        "          performOnSet.add(col)\n",
        "\n",
        "      performOnList = list(performOnSet)\n",
        "      X_ = X[performOnList]\n",
        "      self.means = np.mean(X_, axis=0)\n",
        "      self.stds = np.std(X_, axis=0) \n",
        "\n",
        "      return self\n",
        "\n",
        "    def transform(self, X, y = None):\n",
        "      X_ = X.drop(columns=self.excludeColumns)\n",
        "\n",
        "      X_ = (X_ - self.means) / self.stds\n",
        "\n",
        "      for col in self.excludeColumns:\n",
        "        X_[col] = X[col]\n",
        "        \n",
        "      return X_\n"
      ],
      "metadata": {
        "id": "NVB1p_GnF0S6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reconfigure pipelines\n",
        "cat_cols = ['Quarter']\n",
        "pipelines: Dict[str, skl.pipeline.Pipeline] = {}\n",
        "for stock in targetStocks:\n",
        "  cat_pipeline = Pipeline(steps=[('onehot_cat', OneHotEncoder())])\n",
        "  preproc = ColumnTransformer([('cat_pipe', cat_pipeline, cat_cols)], remainder='passthrough')\n",
        "\n",
        "\n",
        "\n",
        "  pipe = Pipeline(steps=[\n",
        "        ('feature_eng', featureEngPipelines[stock]),\n",
        "        ('cleanup', RemoveBadRowsTransformer()), \n",
        "        ('use_standard_scalar', ExcludeColsStandardScaler(cat_cols)),\n",
        "        ('remove_corr_cols', RemoveHighlyCorrCols(float(.99))),\n",
        "        # ('preproc', preproc), \n",
        "      ])\n",
        "  pipelines[stock] = pipe\n",
        "\n",
        "# # visualization of the pipeline\n",
        "# with config_context(display='diagram'):\n",
        "#     display(pipe)"
      ],
      "metadata": {
        "id": "X2ximTPuAE1T"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msft_df = pipelines[MSFT].fit_transform(initial_exchange)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Poy3YoFwPNNY",
        "outputId": "ccd63e42-feef-4b47-e012-4834e5402b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corr = msft_df.corr()\n",
        "# corr.style.background_gradient(cmap='coolwarm')"
      ],
      "metadata": {
        "id": "7gAn6RwCSzSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = msft_df.drop(columns=['MSFT_WouldProfit'])\n",
        "y = msft_df[['MSFT_WouldProfit']]\n",
        "\n",
        "\n",
        "print(\n",
        "    X.columns[X.isna().any()].tolist(),\n",
        "    y.columns[y.isna().any()].tolist(),\n",
        "\n",
        "    X.columns[X.isnull().any()],\n",
        "    y.columns[y.isnull().any()],\n",
        "  \n",
        "    X.columns[np.isinf(X).any()],\n",
        "    # y.columns[np.isinf(y).any()],\n",
        ")"
      ],
      "metadata": {
        "id": "Ti2MTnArtl9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(msft_df.head(50).to_string())"
      ],
      "metadata": {
        "id": "DrEN54res0y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training our model\n",
        "\n"
      ],
      "metadata": {
        "id": "4OIqjmc0_ox_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c9e1b11"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = 'MSFT_WouldProfit'\n",
        "X = msft_df.drop(columns=[target, 'Quarter'])\n",
        "y = msft_df[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.70)\n",
        "\n",
        "lr = LogisticRegression(class_weight={})\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "print(lr.predict(X_test))\n",
        "\n",
        "print(print(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "print('Accuracy of LR classifier on training set: {:.2f}'.format(lr.score(X_train, y_train)))\n",
        "print('Accuracy of LR classifier on test set: {:.2f}'.format(lr.score(X_test, y_test)))\n",
        "print(classification_report(y_test, lr.predict(X_test)))\n",
        "\n",
        "cm = confusion_matrix(y_test, lr.predict(X_test))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b7sRtM9mkHPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}